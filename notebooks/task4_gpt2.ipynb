{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import einops\n",
    "# Imports for displaying vis in Colab / notebook\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# For the most part I'll try to import functions and classes near where they are used\n",
    "# to make it clear where they come from.\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# utility to clear variables out of the memory & and clearing cuda cache\n",
    "import gc\n",
    "def clear_cache():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model to work with \n",
    "GEMMA = False\n",
    "\n",
    "if GEMMA == True:\n",
    "    BASE_MODEL = \"google/gemma-2b\"\n",
    "    FINETUNE_MODEL = 'shahdishank/gemma-2b-it-finetune-python-codes'\n",
    "    DATASET_NAME = \"ctigges/openwebtext-gemma-1024-cl\"\n",
    "else:\n",
    "    BASE_MODEL = \"gpt2-small\"\n",
    "    FINETUNE_MODEL = 'pierreguillou/gpt2-small-portuguese'\n",
    "    DATASET_NAME = \"Skylion007/openwebtext\"\n",
    "\n",
    "layer_num = 6\n",
    "hook_part = \"pre\"\n",
    "TOTAL_BATCHES = 500\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1 Pretrained case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from saetuning.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the LLM\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "base_model = HookedSAETransformer.from_pretrained(BASE_MODEL, device=device, dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required libraries\n",
    "from sae_lens import SAE\n",
    "\n",
    "# define the SAE id\n",
    "sae_id = f\"blocks.{layer_num}.hook_resid_{hook_part}\"\n",
    "# load the SAE model\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "        release = 'gpt2-small-res-jb',\n",
    "        sae_id = sae_id, # in the case of GPT-2 res SAEs, it coincides with the hook name\n",
    "        device = device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this must be checked for the forward method of sae.encode_xxx\n",
    "cfg_dict[\"activation_fn_str\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens import ActivationsStore\n",
    "\n",
    "# a convenient way to instantiate an activation store is to use the from_sae method\n",
    "activation_store = ActivationsStore.from_sae(\n",
    "    model=base_model,\n",
    "    sae=sae,\n",
    "    streaming=True,\n",
    "    # dataset=chess_dataset,\n",
    "    # fairly conservative parameters here so can use same for larger\n",
    "    # models without running out of memory.\n",
    "    store_batch_size_prompts=8,\n",
    "    train_batch_size_tokens=4096,\n",
    "    n_batches_in_buffer=32,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 L0 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:40<00:00, 12.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# L0_loss(x, threshold=1e-8)\n",
    "# get_substitution_loss(tokens, model, sae, sae_layer)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "all_tokens_L0 = []  # This will store the tokens for reuse\n",
    "all_L0 = []\n",
    "\n",
    "for k in tqdm(range(TOTAL_BATCHES)):\n",
    "    # Get a batch of tokens from the dataset\n",
    "    tokens = activation_store.get_batch_tokens()  # [N_BATCH, N_CONTEXT]\n",
    "    \n",
    "    # Store tokens for later reuse\n",
    "    all_tokens.append(tokens)\n",
    "    \n",
    "    # Run the model and store the activations \n",
    "    _, cache = base_model.run_with_cache(tokens, stop_at_layer=layer_num + 1, \\\n",
    "                                         names_filter=[sae_id])  # [N_BATCH, N_CONTEXT, D_MODEL]\n",
    "    \n",
    "    # Get the activations from the cache at the sae_id \n",
    "    original_activations = cache[sae_id]\n",
    "    \n",
    "    # Encode the activations with the SAE\n",
    "    feature_activations = sae.encode_standard(original_activations) # the result of the encode method of the sae on the \"sae_id\" activations (a specific activation tensor of the LLM) \n",
    "\n",
    "    feature_activations.to('cpu')\n",
    "    \n",
    "    # Store the encoded activations\n",
    "    all_L0.append(L0_loss(feature_activations))\n",
    "\n",
    "    # Explicitly free up memory by deleting the cache and emptying the CUDA cache\n",
    "    del cache\n",
    "    del original_activations\n",
    "    del feature_activations\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Concatenate all tokens into a single tensor for reuse\n",
    "all_tokens_L0 = torch.cat(all_tokens_L0)  # [TOTAL_BATCHES * N_BATCH, N_CONTEXT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(50.3976)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(all_L0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 Substitution Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:34<00:00,  3.24it/s]\n"
     ]
    }
   ],
   "source": [
    "# L0_loss(x, threshold=1e-8)\n",
    "# get_substitution_loss(tokens, model, sae, sae_layer)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "all_tokens_SL = []  # This will store the tokens for reuse\n",
    "all_SL = []\n",
    "\n",
    "for k in tqdm(range(TOTAL_BATCHES)):\n",
    "    # Get a batch of tokens from the dataset\n",
    "    tokens = activation_store.get_batch_tokens()  # [N_BATCH, N_CONTEXT]\n",
    "    \n",
    "    # Store tokens for later reuse\n",
    "    all_tokens_SL.append(tokens)\n",
    "    \n",
    "    # Store loss \n",
    "    all_SL.append(get_substitution_loss(tokens, base_model, sae, sae_id))\n",
    "\n",
    "# Concatenate all tokens into a single tensor for reuse\n",
    "all_tokens = torch.cat(all_tokens_SL)  # [TOTAL_BATCHES * N_BATCH, N_CONTEXT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.6367, dtype=torch.float16)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(all_SL).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1 FineTuned case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from saetuning.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the LLM\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "base_model = HookedSAETransformer.from_pretrained(BASE_MODEL, device=device, dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1680e10e60e74fc295b3a2c5936486f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/92.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d908c8346940e28421f99ccaa5267d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/850k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9e3f86c9d184816b150f26cc435e9f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/508k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcecfd524c09463eb3720c792ab0ebeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/120 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/sae/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='pierreguillou/gpt2-small-portuguese', vocab_size=50257, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the finetune model and its tokenizer\n",
    "finetune_tokenizer = AutoTokenizer.from_pretrained(FINETUNE_MODEL)\n",
    "finetune_model_hf = AutoModelForCausalLM.from_pretrained(FINETUNE_MODEL)\n",
    "finetune_model = HookedSAETransformer.from_pretrained(BASE_MODEL, device=device, hf_model=finetune_model_hf, dtype=torch.float16)\n",
    "\n",
    "finetune_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/sae/lib/python3.10/site-packages/sae_lens/sae.py:136: UserWarning: \n",
      "This SAE has non-empty model_from_pretrained_kwargs. \n",
      "For optimal performance, load the model like so:\n",
      "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# import the required libraries\n",
    "from sae_lens import SAE\n",
    "\n",
    "# define the SAE id\n",
    "sae_id = f\"blocks.{layer_num}.hook_resid_{hook_part}\"\n",
    "# load the SAE model\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "        release = 'gpt2-small-res-jb',\n",
    "        sae_id = sae_id, # in the case of GPT-2 res SAEs, it coincides with the hook name\n",
    "        device = device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'relu'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this must be checked for the forward method of sae.encode_xxx\n",
    "cfg_dict[\"activation_fn_str\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Dataset is not tokenized. Pre-tokenizing will improve performance and allows for more control over special tokens. See https://jbloomaus.github.io/SAELens/training_saes/#pretokenizing-datasets for more info.\n"
     ]
    }
   ],
   "source": [
    "from sae_lens import ActivationsStore\n",
    "\n",
    "# a convenient way to instantiate an activation store is to use the from_sae method\n",
    "activation_store = ActivationsStore.from_sae(\n",
    "    model=base_model,\n",
    "    sae=sae,\n",
    "    streaming=True,\n",
    "    # dataset=chess_dataset,\n",
    "    # fairly conservative parameters here so can use same for larger\n",
    "    # models without running out of memory.\n",
    "    store_batch_size_prompts=8,\n",
    "    train_batch_size_tokens=4096,\n",
    "    n_batches_in_buffer=32,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 L0 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:39<00:00, 12.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# L0_loss(x, threshold=1e-8)\n",
    "# get_substitution_loss(tokens, model, sae, sae_layer)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "all_tokens_L0 = []  # This will store the tokens for reuse\n",
    "all_L0 = []\n",
    "\n",
    "for k in tqdm(range(TOTAL_BATCHES)):\n",
    "    # Get a batch of tokens from the dataset\n",
    "    tokens = activation_store.get_batch_tokens()  # [N_BATCH, N_CONTEXT]\n",
    "    \n",
    "    # Store tokens for later reuse\n",
    "    all_tokens_L0.append(tokens)\n",
    "    \n",
    "    # Run the model and store the activations \n",
    "    _, cache = finetune_model.run_with_cache(tokens, stop_at_layer=layer_num + 1, \\\n",
    "                                         names_filter=[sae_id])  # [N_BATCH, N_CONTEXT, D_MODEL]\n",
    "    \n",
    "    # Get the activations from the cache at the sae_id \n",
    "    original_activations = cache[sae_id]\n",
    "    \n",
    "    # Encode the activations with the SAE\n",
    "    feature_activations = sae.encode_standard(original_activations) # the result of the encode method of the sae on the \"sae_id\" activations (a specific activation tensor of the LLM) \n",
    "\n",
    "    feature_activations.to('cpu')\n",
    "    \n",
    "    # Store the encoded activations\n",
    "    all_L0.append(L0_loss(feature_activations))\n",
    "\n",
    "    # Explicitly free up memory by deleting the cache and emptying the CUDA cache\n",
    "    del cache\n",
    "    del original_activations\n",
    "    del feature_activations\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Concatenate all tokens into a single tensor for reuse\n",
    "all_tokens_L0 = torch.cat(all_tokens_L0)  # [TOTAL_BATCHES * N_BATCH, N_CONTEXT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(74.1725)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(all_L0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Substitution Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:35<00:00,  3.21it/s]\n"
     ]
    }
   ],
   "source": [
    "# L0_loss(x, threshold=1e-8)\n",
    "# get_substitution_loss(tokens, model, sae, sae_layer)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "all_tokens_SL = []  # This will store the tokens for reuse\n",
    "all_SL = []\n",
    "\n",
    "for k in tqdm(range(TOTAL_BATCHES)):\n",
    "    # Get a batch of tokens from the dataset\n",
    "    tokens = activation_store.get_batch_tokens()  # [N_BATCH, N_CONTEXT]\n",
    "    \n",
    "    # Store tokens for later reuse\n",
    "    all_tokens_SL.append(tokens)\n",
    "    \n",
    "    # Store loss \n",
    "    all_SL.append(get_substitution_loss(tokens, finetune_model, sae, sae_id))\n",
    "\n",
    "# Concatenate all tokens into a single tensor for reuse\n",
    "all_tokens = torch.cat(all_tokens_SL)  # [TOTAL_BATCHES * N_BATCH, N_CONTEXT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.1172, dtype=torch.float16)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(all_SL).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
